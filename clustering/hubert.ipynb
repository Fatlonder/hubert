{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python hubert/learn_kmeans.py /content/km 0 1 /content/km/kmeans_model.ext 100 --percent 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "import joblib\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dir_path = f\"/content/mfcc_features\"\n",
    "columns_to_load = [\"features\"]\n",
    "dataset = ds.dataset(feat_dir_path, format=\"parquet\")\n",
    "df = dataset.to_table(columns=columns_to_load).to_pandas().to_numpy()\n",
    "for i in range(df.shape[0]):\n",
    "    df[i, 0] = df[i, 0].reshape(-1, 39)\n",
    "\n",
    "training_data = np.vstack([df[i, 0] for i in range(df.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 100\n",
    "batch_size = 2 # experiment with ones that fit better \n",
    "init = \"k-means++\"\n",
    "max_iter = 100\n",
    "max_no_improvement = 100\n",
    "reassignment_ratio = 0\n",
    "tol = 0.0\n",
    "n_init = 20\n",
    "knn = MiniBatchKMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    init=init,\n",
    "    max_iter=max_iter,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    "    compute_labels=False,\n",
    "    tol=tol,\n",
    "    max_no_improvement=max_no_improvement,\n",
    "    init_size=None,\n",
    "    n_init=n_init,\n",
    "    reassignment_ratio=reassignment_ratio,\n",
    ")\n",
    "\n",
    "print(training_data.shape)\n",
    "\n",
    "knn.fit(training_data)\n",
    "\n",
    "km_path = \"/content/knn_100.model\"\n",
    "joblib.dump(knn, km_path)\n",
    "\n",
    "inertia = -knn.score(training_data) / len(training_data)\n",
    "print(f\"total intertia: {inertia}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python hubert/dump_km_label.py /content/km 0 /content/km/dm.ext 1 0 /content/km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplyKmeans(object):\n",
    "    def __init__(self, km_path):\n",
    "        self.km_model = joblib.load(km_path)\n",
    "        self.C_np = self.km_model.cluster_centers_.transpose()\n",
    "        self.Cnorm_np = (self.C_np ** 2).sum(0, keepdims=True)\n",
    "\n",
    "    def __call__(self, x):\n",
    "          x = x.reshape(1, -1)\n",
    "          dist = ((x ** 2).sum(1, keepdims=True) - 2 * np.matmul(x, self.C_np) + self.Cnorm_np)\n",
    "          return np.argmin(dist, axis=1)\n",
    "\n",
    "km_path = \"/content/knn_100.model\"\n",
    "lab_dir = \"/content/labels\"\n",
    "lab_path = f\"{lab_dir}/knn100.km\"\n",
    "feat_dir = \"/content/mfcc_features\"\n",
    "columns_to_load = [\"features\"]\n",
    "\n",
    "apply_kmeans = ApplyKmeans(km_path)\n",
    "dataset = ds.dataset(feat_dir, format=\"parquet\")\n",
    "df = dataset.to_table(columns=columns_to_load).to_pandas().to_numpy()\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    df[i, 0] = df[i, 0].reshape(-1, 39)\n",
    "\n",
    "mfcc_features_np = np.vstack([df[i, 0] for i in range(df.shape[0])])\n",
    "\n",
    "with open(lab_path, mode=\"w\", newline=\"\") as file:    \n",
    "    for row in mfcc_features_np:\n",
    "        labels = apply_kmeans(row).tolist()\n",
    "        file.write(\" \".join(map(str, labels)) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
